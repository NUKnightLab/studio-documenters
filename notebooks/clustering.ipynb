{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring documents with K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the document set\n",
    "\n",
    "We will use a subset of the documents. Clustering is not likely to yield interesting results for the entire dataset. Before moving on, use the `tools.subset` tool or other means to create a data subset specified by document titles or other metadata.\n",
    "\n",
    "Here, we'll define a generator to yield all the documents of the subset so we can avoid too much memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '../data/DocumentCloud/subset'\n",
    "\n",
    "def documents(datadir=DATADIR):\n",
    "    for fn in os.listdir(datadir):\n",
    "        yield open(os.path.join(datadir, fn)).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs(substring, limit=None):\n",
    "    \"\"\"A crude document search utility\"\"\"\n",
    "    count = 0\n",
    "    for doc in documents():\n",
    "        if substring.lower() in doc.lower():\n",
    "            count += 1\n",
    "            yield doc\n",
    "            if limit is not None and count >= limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to experiment with even more specific subsets of the data. The above defined `find_docs` was just the simplest thing to do ... you might find other approaches to subsetting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_string = ''\n",
    "\n",
    "if search_string:\n",
    "    docs = [doc for doc in find_docs(search_string)]\n",
    "else:\n",
    "    # this could get heavy if you have a lot of docs in your exploration set\n",
    "    docs = [doc for doc in documents()]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000 # the number of features to extract. Essentially, (I think) the size of the\n",
    "            # lexicon, although a small number will lead to hash collisions\n",
    "    \n",
    "k = 5 # the number of clusters to find. You will want to experiment with this number for\n",
    "      # each document set you explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the document text\n",
    "\n",
    "The [HashingVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) will convert a collection of text documents to a matrix of token occurrences. This effectively tokenizes the text and vectorizes in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingVectorizer(\n",
    "    n_features=n, stop_words='english',\n",
    "    alternate_sign=False, norm=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) will convert the document vectors to tf-idf.\n",
    "\n",
    "tf-idf is a term weighting approach that scores the relative importance of terms in a document. For more info, see the [Wikipedia entry](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=n,\n",
    "    min_df=2, stop_words='english', use_idf=True)\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now for our feature matrix `X` is document term vectors with weightings for the \"importance\" of a term. The shape of X will be the document-count x features-extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 3385)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors are sparse matrices, which makes the data representation much lighter given that any given document will only have a small subset of the extracted terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3385 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 156 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the documents\n",
    "\n",
    "Now we are ready to cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = MiniBatchKMeans(\n",
    "    n_clusters=k, init='k-means++', n_init=1,\n",
    "    init_size=1000, batch_size=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
       "                init_size=1000, max_iter=100, max_no_improvement=10,\n",
       "                n_clusters=5, n_init=1, random_state=None,\n",
       "                reassignment_ratio=0.01, tol=0.0, verbose=False)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `km` now contains the clustering information. By looking at the labels that the model has applied to the documents, you can see that there are `k` clusters, with a cluster ID associated with each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, 1, 2, 3, 0, 1, 3, 3, 4, 3, 1, 2, 2, 4, 2, 4, 0, 1, 1, 2,\n",
       "       4, 3, 3, 3, 0, 1, 3, 3, 1, 4, 2, 2, 4, 0, 1, 3, 3, 1, 3, 0, 4, 4,\n",
       "       0, 2, 1, 4, 1, 1, 2, 3, 2, 4, 2, 2, 4, 3, 4, 0, 0, 4, 3, 3, 0, 2,\n",
       "       1, 3, 1, 4, 2, 0, 3, 3, 3, 0, 1, 3, 1, 4, 4, 0, 4, 1, 4, 1, 0, 4,\n",
       "       1, 2, 2, 3, 0, 2, 3, 3, 1, 4, 1, 4, 4, 1, 0, 3, 1, 3, 0, 3, 4, 0,\n",
       "       3, 0, 3, 3, 4, 4, 2, 4, 1, 2, 2, 0, 1, 4, 0, 4, 0, 2, 4, 3, 0, 2,\n",
       "       2, 1, 3, 3, 4, 0, 2, 4, 4, 1, 3, 2, 3, 3, 1, 3, 1, 2, 4, 1, 3, 1,\n",
       "       4, 2, 1, 3, 2, 3, 4, 4, 1, 0, 2, 0, 2, 3, 3, 2, 0, 1, 3, 3, 0, 2,\n",
       "       1, 3, 4, 3, 2, 4, 0, 1, 4, 2, 0, 2, 1, 1, 2, 4, 2, 3, 4, 0, 2, 3,\n",
       "       3, 0, 1, 0, 4, 1, 3, 4, 1, 1, 4, 4, 2, 2, 3, 1, 0, 4, 1, 4, 1, 4,\n",
       "       3, 1, 2, 2, 3, 2, 4, 1, 4, 0, 2, 4, 2, 4, 1, 4, 2, 1, 3, 1, 1, 2,\n",
       "       2, 1, 4, 2, 1, 2, 3, 3, 4, 0, 0, 4, 3, 3, 2, 2, 0, 3, 4, 4, 3, 0,\n",
       "       2, 0, 1, 3, 3, 2, 0, 0, 2, 2, 3, 0, 3, 1, 2, 3, 1, 3, 1, 2, 4, 2,\n",
       "       4, 2, 4, 4, 0, 4, 2, 3, 1, 4, 3, 2, 2, 2, 1, 1, 4, 2, 3, 2, 0, 1,\n",
       "       3, 4, 1, 2, 0, 4, 2, 4, 2, 1, 4, 3, 4, 4, 1, 1, 2, 2, 1, 4, 3, 4,\n",
       "       1, 3, 1, 3, 4, 1, 2, 1, 4, 2, 4, 1, 4, 3, 2, 3, 1, 4, 3, 2, 2, 3,\n",
       "       4, 2, 4, 3, 2, 2, 2, 3, 3, 0, 0, 2, 4, 1, 2, 0, 3, 3, 1, 3, 1, 4,\n",
       "       1, 1, 1, 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the clusters\n",
    "\n",
    "To get an intuitive sense of what is in each cluster, we can extract the top terms from each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster # | Doc count | Terms\n",
      "0 | 48 | stations phone accommodation denied asap efficiencies month corner afternoon requests people accessibility 60661 vs reference processing clock station paid additional facilitator elevator reasonable announcements jefferson information provided intake advisory plan total tuesday including training se occurred totaled complaints representative customer davis escalator 2016 headquarters request streets\n",
      "1 | 78 | coverage participants payment results specialist 22 litigation cancelled account conference bills december premium healthcare supplemental 17 16 compensation discuss room 23 cancellation 18 pm special 21 given irs application deferred hardship performance nd 24 education 27 retirement underpayments 25 investment 28 potential market rescheduled 19 preliminary administration 20 expense employee pending thursday scheduled 2nd 26\n",
      "2 | 82 | relating projects contact listed open called policy terry related legal estate grant location matters aserpe range long operational access real issues capital immediately accommodations american personnel physically strategies\n",
      "3 | 89 | number youngblood bowen presented present rosales th presentations absent seconded reviewed posted moved asked miller stated noticed yes numbers unanimous items item fuller recommend following directors vice discussed approved agendas chief alva ordinances 13th vote johnny monthly votes recommended placed notices director commenced arabel action concluded www voice omnibus\n",
      "4 | 83 | period change license bank furnish maintenance express lease year route city services insurance infrastructure avenue options space promotional america park property modernization _____________ north 36 technology purple located required racine intergovernmental fab 31 option ________ communications provide execution months management llc project acquisition 2020 date experiment years amendment red sublease 31st\n"
     ]
    }
   ],
   "source": [
    "print(\"Cluster # | Doc count | Terms\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "topics = []\n",
    "\n",
    "for i in range(k):\n",
    "    topic = []\n",
    "    for ind in order_centroids[i, :100]:\n",
    "        topic.append(terms[ind])\n",
    "    topics.append(set(topic))\n",
    "    \n",
    "for i, t in enumerate(topics):\n",
    "    for j, t2 in enumerate(topics):\n",
    "        if i == j:\n",
    "            continue\n",
    "        t = t - t2\n",
    "    print(i, list(km.labels_).count(i), ' '.join(t), sep=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given that one of these clusters looks interesting, you might want to take a look at the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 3\n",
    "indexes = [i for i,v in enumerate(km.labels_) if v==3]\n",
    "cluster_docs = [docs[i] for i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINUTES: Finance Audit and Budget Committee. thOctober\n",
      ", 2018. 10\n",
      "NOTICED: Following earlier committee. Commenced: 9:24 AM.\n",
      "\n",
      "AGENDA: The posted agenda for the meeting can be found at www.transitchicago.com\n",
      ", “About CTA”,\n",
      "“Transit Board Meetings”, “Meeting Notices, Agendas, and Minutes”, “10/10/2018”, “Committee o\n",
      "Finance Audit, and Budget.\n",
      "\n",
      "ROLL CALL: Chairman Silva, Irvine, Peterson, Miller, Patterson, Youngblood. There was a quorum\n",
      "committee members present, and one (Alva Rosales) absent.\n",
      "\n",
      "COMMITTEE ACTION: The committee reviewed the Finance report and approved th\n",
      "the\n",
      ", September 1\n",
      "2018 committee minutes.\n",
      "\n",
      "Then, after extensive review by the committee, Chairman Silva asked for a motion to place all\n",
      "recommended approved items, the four ordinances and the 14 contracts, on the omnibus for Boar\n",
      "approval. Moved and seconded, the motion to recommend Board approval of the omnibus was\n",
      "approved with six yes votes.\n",
      "The approved items are as follows:\n",
      "\n",
      "1 An ordinance authorizing an intergovernmental agreement with the City of Evanston for fun\n",
      "from the Washington National TIF District for future phases of the Red and Purple\n",
      "Modernization Program.\n",
      "2 An ordinance establishing the Authority’s Disadvantaged Business Enterprise (DBE) program\n",
      "goal for the Federal Fiscal Years 2019-2021.\n",
      "3 An ordinance revising the Authority’s Drug and Alcohol Policy and Testing Program for\n",
      "employees in safety-sensitive positions.\n",
      "4 An ordinance authorizing the purchase of blanket railroad protective liability insurance for p\n",
      "year November 1, 2018 through October 31, 2019.\n",
      "5 Contract Number C18FT102173329: $1,731,516.00\n",
      "6 Contract Number C18CT102180638: $1,170,077.78\n",
      "7 Contract Number C18CT102072380: $225,000.00\n",
      "8 Contract Number B18OP00808: $230,000.00\n",
      "9 Contract Number B13OP03704: $618,604.00\n",
      "10 Contract Number C13FI101442777: $19,080,541.32\n",
      "11 Contract Number B12OP03443: $0.00 (3 month time extension)\n",
      "12 Contract Number B12RG03792A: $77,000.00 (estimated revenue for 3 month extension)\n",
      "13 Contract Number B18OP01980: $125,000,000.00\n",
      "14 Contract Number B14RT04137: $12,000.00\n",
      "15 Contract Number B18OP04075: $600,000.00\n",
      "16 Contract Number B15OP03691: $172,000.00\n",
      "17 Contract Number B18FT04541: $32,745,699.00\n",
      "18 Contract Number C14FI101542505: $1,273,967.00\n",
      "1\n",
      "\n",
      " MOTION TO ADJOURN: Chairman Silva asked for a motion to adjourn the Finance, Audit, and Budg\n",
      "th\n",
      "committee meeting of October\n",
      ", 2018.\n",
      "10 After the motion was moved, the motion was approved by\n",
      "unanimous voice vote.\n",
      "\n",
      "2\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(cluster_docs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
